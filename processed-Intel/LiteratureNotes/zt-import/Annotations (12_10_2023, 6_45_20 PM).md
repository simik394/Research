# Annotations  
(12/10/2023, 6:45:20 PM)

> [!#fff066] [Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model](zotero://open-pdf/library/items/JPNZN3UM?page=1&annotation=JST8ZF8R) Page 1
> 
> ---
> 6 tezi
> ^JST8ZF8RaJPNZN3UMp1

[@schrittwieserMasteringAtariGo2020] tkkk

> [!#8fdef9] [Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artiﬁcial intelligence](zotero://open-pdf/library/items/JPNZN3UM?page=1&annotation=V8YIGTFF) Page 1
> ^V8YIGTFFaJPNZN3UMp1

> [!#8fdef9] [Tree-based planning methods have enjoyed huge success in challeng-](zotero://open-pdf/library/items/JPNZN3UM?page=1&annotation=AWMG2LJI) Page 1
> ^AWMG2LJIaJPNZN3UMp1

> [!#8fdef9] [ing domains, such as chess and Go, where a perfect simulator is available](zotero://open-pdf/library/items/JPNZN3UM?page=1&annotation=YRUPS9HX) Page 1
> ^YRUPS9HXaJPNZN3UMp1

> [!#eb4949] [, in real-world problems the dynamics governing the environment are often complex and unknown.](zotero://open-pdf/library/items/JPNZN3UM?page=1&annotation=NP95AIVZ) Page 1
> ^NP95AIVZaJPNZN3UMp1

> [!#7df066] [MuZero algorithm](zotero://open-pdf/library/items/JPNZN3UM?page=1&annotation=436YLY9V) Page 1
> 
> ---
> - MuZero algorithm
> - which, by combining a tree-based search with a learned model,
> achieves superhuman performance in a range of challenging and visually complex domains, without
> any knowledge of their underlying dynamics.
> ^436YLY9VaJPNZN3UMp1

> [!#fff066] [MuZero](zotero://open-pdf/library/items/JPNZN3UM?page=1&annotation=2QAPTPEK) Page 1
> 
> ---
> - MuZero - learns a model that, when applied iteratively,
> predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and
> the value function.
> ^2QAPTPEKaJPNZN3UMp1

> [!#f799d1] [When evaluated on Go, chess and shogi, withoutany knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZeroalgorithm that was supplied with the game rules.](zotero://open-pdf/library/items/JPNZN3UM?page=1&annotation=RPYMGYKC) Page 1
> ^RPYMGYKCaJPNZN3UMp1

> [!#f799d1] [When evaluated on Go, chess and shogi, withoutany knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZeroalgorithm that was supplied with the game rules.](zotero://open-pdf/library/items/JPNZN3UM?page=1&annotation=DQXLH3ZX) Page 1
> ^DQXLH3ZXaJPNZN3UMp1

> [!#fff066] [Planning algorithms based on lookahead search](zotero://open-pdf/library/items/JPNZN3UM?page=1&annotation=5PEGJWPJ) Page 1
> ^5PEGJWPJaJPNZN3UMp1

> [!#8fdef9] [Planning algorithms based on lookahead search have achieved remarkable successes in artiﬁcial intelligence. Human world champions have been defeated in classic games such as checkers [34], chess [5], Go [38] and poker [3, 26], and planning algorithms have had real-world impact in applications from logistics [47] to chemical synthesis [37]. However, these planning algorithms all rely on knowledge of the environment’s dynamics, such as the](zotero://open-pdf/library/items/JPNZN3UM?page=1&annotation=TC6NY3P5) Page 1
> ^TC6NY3P5aJPNZN3UMp1

> [!#fff066] [knowledge of the environment’s dynamics](zotero://open-pdf/library/items/JPNZN3UM?page=1&annotation=ZJTNKVZU) Page 1
> ^ZJTNKVZUaJPNZN3UMp1

> [!#fff066] [Model-based reinforcement learning (RL)](zotero://open-pdf/library/items/JPNZN3UM?page=1&annotation=7XYKKFP4) Page 1
> 
> ---
> - Model-based RL - aims to address this issue by first learning a model of the
> environment’s dynamics, and then planning with respect to the learned model.
> ^7XYKKFP4aJPNZN3UMp1

> [!#8fdef9] [Typically, these models have either focused on reconstructing the true environmental state [8, 16, 24], or the sequence of full observations [14, 20]](zotero://open-pdf/library/items/JPNZN3UM?page=1&annotation=ZE5J9KBC) Page 1
> ^ZE5J9KBCaJPNZN3UMp1

> [!#8fdef9] [However, prior work [4, 14, 20] remains far from the state of the art in visually rich domains, such as Atari 2600 games [2].](zotero://open-pdf/library/items/JPNZN3UM?page=1&annotation=UFE97EAY) Page 1
> ^UFE97EAYaJPNZN3UMp1

> [!#8fdef9] [Instead, the most successful methods are based on model-free RL [9, 21, 18]](zotero://open-pdf/library/items/JPNZN3UM?page=1&annotation=4U8FMG8J) Page 1
> ^4U8FMG8JaJPNZN3UMp1

> [!#fff066] [model-free](zotero://open-pdf/library/items/JPNZN3UM?page=1&annotation=IS8QLVQP) Page 1
> 
> ---
> - model-free RL – i.e. they estimate
> the optimal policy and/or value function directly from interactions with the environment. However, model-free
> algorithms are in turn far from the state of the art in domains that require precise and sophisticated lookahead, such
> as chess and Go.
> ^IS8QLVQPaJPNZN3UMp1

> [!#fff066] [model-based](zotero://open-pdf/library/items/JPNZN3UM?page=2&annotation=7X7TVQWY) Page 2
> ^7X7TVQWYaJPNZN3UMp2

> [!#fff066] [model-free](zotero://open-pdf/library/items/JPNZN3UM?page=2&annotation=I7ABVUBE) Page 2
> ^I7ABVUBEaJPNZN3UMp2

> [!#fff066] [Markov-decision process (MDP)](zotero://open-pdf/library/items/JPNZN3UM?page=2&annotation=2NRVFKJQ) Page 2
> ^2NRVFKJQaJPNZN3UMp2

> [!#fff066] [MDP planning algorithms](zotero://open-pdf/library/items/JPNZN3UM?page=2&annotation=QCG6NDEP) Page 2
> 
> ---
> - MDP planning algorithms - such as value iteration
> [31] or Monte-Carlo tree search (MCTS) [7
> ^QCG6NDEPaJPNZN3UMp2

> [!#fff066] [Monte-Carlo tree search (MCTS)](zotero://open-pdf/library/items/JPNZN3UM?page=2&annotation=VZ7YDSIL) Page 2
> 
> ---
> - Monte-Carlo tree search (MCTS) - 
>  is performed
> at each timestep t, as described in A. An action at+1 is sampled from the search policy πt, which is proportional
> to the visit count for each action from the root node. The environment receives the action and generates a new
> observation ot+1 and reward ut+1. At the end of the episode the trajectory data is stored into a replay buffer.
> ^VZ7YDSILaJPNZN3UMp2

> [!#fff066] [MDP](zotero://open-pdf/library/items/JPNZN3UM?page=2&annotation=TLB9ZCLW) Page 2
> ^TLB9ZCLWaJPNZN3UMp2

> [!#fff066] [model-based](zotero://open-pdf/library/items/JPNZN3UM?page=2&annotation=Q9U7DBJ8) Page 2
> ^Q9U7DBJ8aJPNZN3UMp2

> [!#8fdef9] [It has been hypothesized that deep, stochastic models may mitigate the problems of compounding error [14, 20]](zotero://open-pdf/library/items/JPNZN3UM?page=2&annotation=RLPW8NNW) Page 2
> ^RLPW8NNWaJPNZN3UMp2

> [!#8fdef9] [However, planning at pixel-level granularity is not computationally tractable in large scale problems. Other methods build a latent state-space model that is sufﬁcient to reconstruct the observation stream at pixel level [48, 49], or to predict its future latent states [13, 11], which facilitates more efﬁcient planning but still focuses the majority of the model capacity on potentially irrelevant detail](zotero://open-pdf/library/items/JPNZN3UM?page=2&annotation=AHY2B6YT) Page 2
> ^AHY2B6YTaJPNZN3UMp2

> [!#8fdef9] [None of these prior methods has constructed a model that facilitates effective planning in visually complex domains such as Atari; results lag behind well-tuned, model-free methods, even in terms of data efﬁciency [45]](zotero://open-pdf/library/items/JPNZN3UM?page=2&annotation=4HTJ9NCX) Page 2
> ^4HTJ9NCXaJPNZN3UMp2

> [!#fff066] [model-based](zotero://open-pdf/library/items/JPNZN3UM?page=2&annotation=3QML5BU4) Page 2
> ^3QML5BU4aJPNZN3UMp2

> [!#fff066] [end-to-end on predicting](zotero://open-pdf/library/items/JPNZN3UM?page=2&annotation=9MG2NZ93) Page 2
> ^9MG2NZ93aJPNZN3UMp2

> [!#fff066] [MDP](zotero://open-pdf/library/items/JPNZN3UM?page=2&annotation=XRBVSFBA) Page 2
> ^XRBVSFBAaJPNZN3UMp2

> [!#fff066] [value equivalence](zotero://open-pdf/library/items/JPNZN3UM?page=2&annotation=J3WHLJ2L) Page 2
> 
> ---
> - value equivalence -- i.e. that, starting from the same real state, the cumulative reward of a trajectory through the
> abstract MDP matches the cumulative reward of a trajectory in the real environment
> ^J3WHLJ2LaJPNZN3UMp2

> [!#fff066] [temporal-difference learning](zotero://open-pdf/library/items/JPNZN3UM?page=2&annotation=NPZ8PGPE) Page 2
> ^NPZ8PGPEaJPNZN3UMp2

> [!#fff066] [Monte-Carlo Tree Search](zotero://open-pdf/library/items/JPNZN3UM?page=3&annotation=HUQQKMX9) Page 3
> ^HUQQKMX9aJPNZN3UMp3

> [!#fff066] [TreeQN](zotero://open-pdf/library/items/JPNZN3UM?page=3&annotation=G9LUUJEY) Page 3
> 
> ---
> - TreeQN - 
> learns an abstract MDP model, such that a tree search over that model (represented by a tree-structured neural network)
> approximates the optimal value function
> ^G9LUUJEYaJPNZN3UMp3

> [!#fff066] [Value iteration networks](zotero://open-pdf/library/items/JPNZN3UM?page=3&annotation=RZCYXKKH) Page 3
> 
> ---
> - Value iteration networks =
> learn a local MDP model, such that value
> iteration over that model (represented by a convolutional neural network) approximates the optimal value function.
> ^RZCYXKKHaJPNZN3UMp3

> [!#fff066] [Value prediction networks](zotero://open-pdf/library/items/JPNZN3UM?page=3&annotation=3V23G2R5) Page 3
> 
> ---
> - Value prediction networks - 
> are perhaps the closest precursor to MuZero: they learn an MDP model grounded
> in real actions; the unrolled MDP is trained such that the cumulative sum of rewards, conditioned on the actual
> sequence of actions generated by a simple lookahead search, matches the real environment. Unlike MuZero there
> is no policy prediction, and the search only utilizes value prediction.
> ^3V23G2R5aJPNZN3UMp3
